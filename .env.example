# LLM Configuration (Ollama only)
LLM_MODEL=qwen3:235b
LLM_TEMPERATURE=0.05
LOCAL_LLM_URL=http://localhost:11434/api/generate
LLM_MAX_TOKENS=2048
# URL for running Ollama hosted model


# NATS Configuration
NATS_SERVER_URL=nats://localhost:4222
# For Docker deployment use: nats://test-nats-server-JS:4222
NATS_STREAM_NAME=AGENT_AI_PIPELINE
NATS_STREAM_PREFIX=agentAI
AUTO_OPEN_CONNECTION=true
DURABLE_NAME=agent_ai_durable
QUEUE_NAME=agent_ai_queue

# NATS Subjects
INPUT_SUBJECT_SUFFIX=.input
ANALYSIS_SUBJECT_SUFFIX=.analysis
OUTPUT_SUBJECT_SUFFIX=.output

# Web App Configuration
WEBAPP_HOST=0.0.0.0
WEBAPP_PORT=5000
WEBAPP_DEBUG=true

# Logging Configuration
LOG_LEVEL=INFO
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s

# Docker Network Commands (for reference):
# docker network create nats-net
# docker network connect nats-net test-nats-server-JS
# docker network connect nats-net your_container_name