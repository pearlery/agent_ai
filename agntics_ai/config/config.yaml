# NATS Configuration
nats:
  server_url: "nats://localhost:4222"
  stream_name: "AGENT_AI_PIPELINE"
  subjects:
    input: "agentAI.input"
    analysis: "agentAI.analysis"
    output: "agentAI.output"

# LLM Configuration (Ollama only)
llm:
  local_url: "http://localhost:11434/api/generate"
  local_model: "qwen3:235b"
  temperature: 0.05
  max_tokens: 2048

webapp:
  host: "0.0.0.0"
  port: 5000
  debug: true

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"