# NATS Configuration
nats:
  server_url: "nats://192.168.55.158:31653"
  stream_name: "agentAI_stream"
  auto_open_connection: false #ทดสอบแบบไม่มีnats
  subjects:
    input: "agentAI.Input"
    analysis: "agentAI.Analysis"
    output: "agentAI.Output"

# LLM Configuration (Ollama only)
llm:
  local_url: "http://192.168.123.110:11434/api/generate"
  local_model: "llama4:128x17b"
  temperature: 0.05
  max_tokens: 2048

webapp:
  host: "0.0.0.0"
  port: 5000
  debug: true

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"