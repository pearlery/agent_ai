# NATS Configuration
nats:
  server_url: "nats://localhost:4222"
  stream_name: "AGENT_AI_PIPELINE"
  auto_open_connection: false #ทดสอบแบบไม่มีnats
  subjects:
    input: "agentAI.Input"
    analysis: "agentAI.Analysis"
    output: "agentAI.Output"
    graphql_mutation: "agentAI.graphql.mutation"

# LLM Configuration (Ollama only)
llm:
  local_url: "http://localhost:11434/api/generate"
  local_model: "llama3.1"
  temperature: 0.05
  max_tokens: 2048

webapp:
  host: "0.0.0.0"
  port: 5000
  debug: true

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"