services:
  # NATS JetStream Server
  nats-server:
    image: nats:2.10-alpine
    container_name: agent-ai-nats-server
    command: ["--jetstream", "--http_port=8222"]
    ports:
      - "4222:4222"    # NATS protocol
      - "8222:8222"    # HTTP monitoring
    volumes:
      - nats-data:/data
    networks:
      - agent-ai-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "4222"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s

  # Agent AI Application
  agent-ai:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: agent-ai-app
    environment:
      # NATS Configuration
      - NATS_SERVER_URL=nats://nats-server:4222
      - NATS_STREAM_NAME=AGENT_AI_PIPELINE
      - AUTO_OPEN_CONNECTION=true
      
      # LLM Configuration (Ollama)
      - LOCAL_LLM_URL=http://ollama:11434/api/generate
      - LLM_MODEL=llama3.1
      - LLM_TEMPERATURE=0.05
      - LLM_MAX_TOKENS=2048
      
      # Web App Configuration
      - WEBAPP_HOST=0.0.0.0
      - WEBAPP_PORT=5000
      - WEBAPP_DEBUG=false
      
      # Logging
      - LOG_LEVEL=INFO
    ports:
      - "5000:5000"    # Web App
      - "9004:9004"    # Control Agent API
    depends_on:
      nats-server:
        condition: service_healthy
      # ollama:
      #   condition: service_started
    networks:
      - agent-ai-network
    restart: unless-stopped
    volumes:
      - ./agntics_ai:/app/agntics_ai:ro
      - ./data:/app/data
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -f http://localhost:5000/health && curl -f http://localhost:9004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Ollama (Optional - for local LLM)
  ollama:
    image: ollama/ollama:latest
    container_name: agent-ai-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - agent-ai-network
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0
    profiles:
      - ollama  # Use with: docker-compose --profile ollama up

  control-agent:
      build: .
      env_file:
        - .env
      depends_on:
        - nats-server
      networks:
        - agent-ai-network
      ports:
        - "8000:8000"
volumes:
  nats-data:
    driver: local
  ollama-data:
    driver: local

networks:
  agent-ai-network:
    driver: bridge
    name: agent-ai-network

